##############################
#DO NOT FORGET:
#1. BOTH DIMS AND MEASURES NEED TO BE CLEAN (hence no non numeric columns)
#2. According to the use case (e.g. if many numeric where clause cols, increase augment_numeric_where_param_range_factor to around 2-3 otherwise 1)
#3. line 548 decide if to filter out queries that does not return value or return 0
{
#'vector_size' : 40,#
'vector_size_growth_factor' : 20, # it will add <vector_size_growth_factor> bits to the calculated required vector size
'negative_number_ind' : False,
# setting a fixed vector size enables the encoder to grow when new members arrives incrementally (a size of 30 is approx. 1B distinct values in the data)
'global_dict_file_name'  : '{"count_UKEY": "encoder_dim_25_rand_ww_churn_avg_UKEY_churn_incremental_learning_date_based.csv__id_2_vec_rep20190422-195056.pickle"}',
'global_vector_space_dict_file_name'  : '{"count_UKEY": "encoder_dim_25_rand_ww_churn_avg_UKEY_churn_incremental_learning_date_based.csv__id_2_vec_size20190422-195056.pickle"}',
'spark_sql_execution' : False,
'spark_prod_end' : False,
'ec_name' : 'incremental_learning_churn',
'num_files' : 1,
's3_path': '',
#'s3_path': 'data_dir/',
'spark_suffix' : '.csv',
'agg_functions' : ["avg"],#,"sum","avg","max"],#, "min", "max", "count"],
'dict_agg_functions' : ["count"],
'num_supported_measure_where' : 1,
'num_supported_dims' : 2,
'num_supported_members' : 1,
#'sample_data_query' : 'select * from %s where date < 1516195082 limit %s ',
'sample_data_query' : 'select * from %s  limit %s ',


'query_limit' : 10000,
'augment_numeric_where_param_range_factor' : 0,
'num_fraction_decimal_figures_allowed' : 0,
'filter_out_empty_df' : True,
'user_queries_table_name' : ['FalseCall_Round'], ## relevant only to user_queries mode (user_queries_training_set_generator_GROUPBY.py)
#'user_queries_table_name' : ['Pass_Round','FalseCall_Round','Defect_Round'],

'ds_name' : 'toy', ## METADATA is CHOSEN ACCORDING TO THIS NAME
'db_table_name' : 'toy',
'ds_name_file_name' : 'toy', # for writing process output files (training sets files)
'num_queries': 500, # approximately only 25% will return results


#'fix_vector_size_ind' : True,
'bootstrap_sample_num' : 20, # don't set too high, it takes time to bootstrap (take about 30 sec for an iteration of 50k queries)
### NUM EFFECTED QUERIES = num_queries * bootstrap_sample_num
'diminish_single_row_result_queries' : False,
'filter_queries_returning_zero' : False,
'mode': 'batch', # 'online' for sisense EC queries or spark queries | 'batch' for local files
#ds_name must be at least contained in the actual data_dir/[file_name] and meta_data_dir/[file_name]
'target_col' : -1,
'max_n_row_per_file' : -1,
'meta_data_dir' : 'meta_data_dir',
'meta_file_name' : '',
'data_dir' : './data',
#'table_name_wrapper_prefix' : '',
#'table_name_wrapper_sufix' : '',
# aggregation mode: 'single' for create aggregation specific training sets, and 'multi' for 1 training set with all aggregations functions
'aggregations_mode' : 'single',
'select_mode' : 'single',
# PLEASE USE ONLY 1 AGG FUNCTION AT A TIME (GLOBAL DICT)
'no_where_prop' : 0,
'raw_data_num_of_rows_per_file' : 1000000,
'sample_size_per_file' : None,
'representation_mode' : 'dim', # usually should be hot_encoding
'pca_ind': True,
'input_scale' : True,
'dim_where_indicator' : True,
# 24/07/2018 Celestica local project
#'dict_agg_functions' : ["count"]
'entropy_flag' : True,

'weighted_sample_method' : 'equal',
'all_members_operator_ind' : False
}
